## 데이터 분석 예제

### Bit.ly의 1.USA.gov 데이터

- .gov나 .mil로 끝나는 URL을 축약한 사용자들에 대한 익명 정보

```python
import json

records = [json.loads(line) for line in open(path, encoding="UTF-8")]

records[0]
# {'a': 'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/535.11 (KHTML, like Gecko) # Chrome/17.0.963.78 Safari/535.11',
#  'c': 'US',
#  'nk': 1,
#  'tz': 'America/New_York',
# 'gr': 'MA',
# 'g': 'A6qOVH',
# 'h': 'wfLQtf',
# 'l': 'orofrog',
# 'al': 'en-US,en;q=0.8',
# 'hh': '1.usa.gov',
# 'r': 'http://www.facebook.com/l/7AQEFzjSi/1.usa.gov/wfLQtf',
# 'u': 'http://www.ncbi.nlm.nih.gov/pubmed/22415991',
# 't': 1331923247,
# 'hc': 1331822918,
# 'cy': 'Danvers',
# 'll': [42.576698, -70.954903]}
```

- json loads 함수를 이용해서 샘플 파일을 한 줄씩 읽어옴

```python
time_zones = [rec['tz'] for rec in records if "tz" in rec]
time_zones[:10]
# ['America/New_York', 'America/Denver', 'America/New_York', 'America/Sao_Paulo',  'America/New_York', 'America/New_York', 'Europe/Warsaw', '', '', '']
```

- record에 "tz" 키가 있는 경우 "tz" 키의 값을 list로 저장

```python
from collections import defaultdict

def get_counts(sequence):
    counts = defaultdict(int)
    for x in sequence:
        counts[x] += 1
    return counts

counts = get_counts(time_zones)
print(counts["America/New_York"])
# 1251
len(time_zones)
# 3440

def top_counts(count_dict, n=10):
    value_key_paris = [(count, tz) for tz, count in count_dict.items()]
    value_key_paris.sort()
    return value_key_paris[-n:]

top_counts(counts)

# 위의 과정을 한번에 가능
# from collections import Counter
# counts = Counter(time_zones)
# counts.most_common(10)

# [(33, 'America/Sao_Paulo'),
#  (35, 'Europe/Madrid'),
#  (36, 'Pacific/Honolulu'),
#  (37, 'Asia/Tokyo'),
#  (74, 'Europe/London'),
#  (191, 'America/Denver'),
#  (382, 'America/Los_Angeles'),
#  (400, 'America/Chicago'),
#  (521, ''),
#  (1251, 'America/New_York')]
```

- 가장 많이 등장하는 상위 10개의 표준시간대를 추출

```python
import pandas as pd

frame = pd.DataFrame(records)

tz_counts = frame['tz'].value_counts()
tz_counts[:10]
# America/New_York       1251
#                         521
# America/Chicago         400
# America/Los_Angeles     382
# America/Denver          191
# Europe/London            74
# Asia/Tokyo               37
# Pacific/Honolulu         36
# Europe/Madrid            35
# America/Sao_Paulo        33

clean_tz = frame["tz"].fillna("Missing")
clean_tz[clean_tz == ""] = "UnKnown"
tz_counts = clean_tz.value_counts()
tz_counts[:10]
# America/New_York       1251
# UnKnown                 521
# America/Chicago         400
# America/Los_Angeles     382
# America/Denver          191
# Missing                 120
# Europe/London            74
# Asia/Tokyo               37
# Pacific/Honolulu         36
# Europe/Madrid            35
```

- Dataframe으로 변환 후 비어있는 값들을 채워넣음 

```python
import seaborn as sns

subset = tz_counts[:10]
sns.barplot(y=subset.index, x=subset.values)
```

- 다음과 같이 시간대 분포를 보임

<img src="https://user-images.githubusercontent.com/58063806/123638143-c7c1fc00-d859-11eb-9cc7-81ab13932195.png" width=60%/>

```python
results = pd.Series([x.split()[0] for x in frame.a.dropna()])
results[:5]
# 0               Mozilla/5.0
# 1    GoogleMaps/RochesterNY
# 2               Mozilla/4.0
# 3               Mozilla/5.0
# 4               Mozilla/5.0

results.value_counts()[:8]
# Mozilla/5.0                 2594
# Mozilla/4.0                  601
# GoogleMaps/RochesterNY       121
# Opera/9.80                    34
# TEST_INTERNET_AGENT           24
# GoogleProducer                21
# Mozilla/6.0                    5
# BlackBerry8520/5.0.0.681       4
```

- a 필드에는 URL 단축을 실행하는 브라우저, 단말기, 어플리케이션에 대한 정보가 들어있음
- 문자열에서 첫 번째 토큰을 잘라내서 사용자 행동에 대한 또 다른 정보를 생성

```python
cframe = frame[frame.a.notnull()]
cframe["os"] = np.where(cframe["a"].str.contains("Windows"), "Windows", "Not Windows")
cframe["os"][:5]
# 0        Windows
# 1    Not Windows
# 2        Windows
# 3    Not Windows
# 4        Windows
```

- a 필드가 결측값이 아닌 데이터를 추출
- agent 문자열에 "Windows"를 포함하는지 여부에 따라 윈도우 사용자와 그렇지 않은 사용자를 구분

```python
by_tz_os = cframe.groupby(["tz", "os"])
# size함수로 그룹별 횟수 count를 진행
agg_counts = by_tz_os.size().unstack().fillna(0)
agg_counts[:10]
```

<img src="https://user-images.githubusercontent.com/58063806/123639090-d2c95c00-d85a-11eb-9f75-37b17438876e.png" width=40% />

```python
indexer = agg_counts.sum(1).argsort()
count_subset = agg_counts.take(indexer[-10:])
count_subset

# agg_counts.sum(1).nlargest(10)
# tz
# America/New_York       1251.0
#                         521.0
# America/Chicago         400.0
# America/Los_Angeles     382.0
# America/Denver          191.0
# Europe/London            74.0
# Asia/Tokyo               37.0
# Pacific/Honolulu         36.0
# Europe/Madrid            35.0
# America/Sao_Paulo        33.0
```

- 사용자의 합계가 많은 순서대로 10개 로우만 추출
  - nlargest 메서드로 동일한 작업이 가능

<img src="https://user-images.githubusercontent.com/58063806/123639381-1328da00-d85b-11eb-9c16-848fb598bf2b.png" width=35% />

```python
count_subset = count_subset.stack()
count_subset.name = "total"
count_subset = count_subset.reset_index()
count_subset[:10]
```

<img src="https://user-images.githubusercontent.com/58063806/123640996-b4fcf680-d85c-11eb-9f7a-41b9db6f6026.png" width=35%/>

```python
sns.barplot(x="total", y="tz", hue="os", data=count_subset)
```

- 각 시간대(tz)의 os별 분포

<img src="https://user-images.githubusercontent.com/58063806/123641288-102ee900-d85d-11eb-8a1d-b1c5a0275e25.png" width=60% />

```python
def norm_total(group):
    group["normed_total"] = group.total / group.total.sum()
    return group

results = count_subset.groupby("tz").apply(norm_total)

# 위의 과정과 동일한 작업
# g = count_subset.groupby("tz")
# results = count_subset.total / g.total.transform("sum")

sns.barplot(x="normed_total", y="tz", hue="os", data=results)
```

- 각 시간대(tz)에서 비율 확인을 위해 0 ~ 1로 정규화

<img src="https://user-images.githubusercontent.com/58063806/123641639-6d2a9f00-d85d-11eb-8389-6cc8ccd3c85f.png" width=60% />