## 데이터 분석 예제

### Bit.ly의 1.USA.gov 데이터

- .gov나 .mil로 끝나는 URL을 축약한 사용자들에 대한 익명 정보

```python
import json

records = [json.loads(line) for line in open(path, encoding="UTF-8")]

records[0]
# {'a': 'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/535.11 (KHTML, like Gecko) # Chrome/17.0.963.78 Safari/535.11',
#  'c': 'US',
#  'nk': 1,
#  'tz': 'America/New_York',
# 'gr': 'MA',
# 'g': 'A6qOVH',
# 'h': 'wfLQtf',
# 'l': 'orofrog',
# 'al': 'en-US,en;q=0.8',
# 'hh': '1.usa.gov',
# 'r': 'http://www.facebook.com/l/7AQEFzjSi/1.usa.gov/wfLQtf',
# 'u': 'http://www.ncbi.nlm.nih.gov/pubmed/22415991',
# 't': 1331923247,
# 'hc': 1331822918,
# 'cy': 'Danvers',
# 'll': [42.576698, -70.954903]}
```

- json loads 함수를 이용해서 샘플 파일을 한 줄씩 읽어옴

```python
time_zones = [rec['tz'] for rec in records if "tz" in rec]
time_zones[:10]
# ['America/New_York', 'America/Denver', 'America/New_York', 'America/Sao_Paulo',  'America/New_York', 'America/New_York', 'Europe/Warsaw', '', '', '']
```

- record에 "tz" 키가 있는 경우 "tz" 키의 값을 list로 저장

```python
from collections import defaultdict

def get_counts(sequence):
    counts = defaultdict(int)
    for x in sequence:
        counts[x] += 1
    return counts

counts = get_counts(time_zones)
print(counts["America/New_York"])
# 1251
len(time_zones)
# 3440

def top_counts(count_dict, n=10):
    value_key_paris = [(count, tz) for tz, count in count_dict.items()]
    value_key_paris.sort()
    return value_key_paris[-n:]

top_counts(counts)

# 위의 과정을 한번에 가능
# from collections import Counter
# counts = Counter(time_zones)
# counts.most_common(10)

# [(33, 'America/Sao_Paulo'),
#  (35, 'Europe/Madrid'),
#  (36, 'Pacific/Honolulu'),
#  (37, 'Asia/Tokyo'),
#  (74, 'Europe/London'),
#  (191, 'America/Denver'),
#  (382, 'America/Los_Angeles'),
#  (400, 'America/Chicago'),
#  (521, ''),
#  (1251, 'America/New_York')]
```

- 가장 많이 등장하는 상위 10개의 표준시간대를 추출

```python
import pandas as pd

frame = pd.DataFrame(records)

tz_counts = frame['tz'].value_counts()
tz_counts[:10]
# America/New_York       1251
#                         521
# America/Chicago         400
# America/Los_Angeles     382
# America/Denver          191
# Europe/London            74
# Asia/Tokyo               37
# Pacific/Honolulu         36
# Europe/Madrid            35
# America/Sao_Paulo        33

clean_tz = frame["tz"].fillna("Missing")
clean_tz[clean_tz == ""] = "UnKnown"
tz_counts = clean_tz.value_counts()
tz_counts[:10]
# America/New_York       1251
# UnKnown                 521
# America/Chicago         400
# America/Los_Angeles     382
# America/Denver          191
# Missing                 120
# Europe/London            74
# Asia/Tokyo               37
# Pacific/Honolulu         36
# Europe/Madrid            35
```

- Dataframe으로 변환 후 비어있는 값들을 채워넣음 

```python
import seaborn as sns

subset = tz_counts[:10]
sns.barplot(y=subset.index, x=subset.values)
```

- 다음과 같이 시간대 분포를 보임

<img src="https://user-images.githubusercontent.com/58063806/123638143-c7c1fc00-d859-11eb-9cc7-81ab13932195.png" width=60%/>

```python
results = pd.Series([x.split()[0] for x in frame.a.dropna()])
results[:5]
# 0               Mozilla/5.0
# 1    GoogleMaps/RochesterNY
# 2               Mozilla/4.0
# 3               Mozilla/5.0
# 4               Mozilla/5.0

results.value_counts()[:8]
# Mozilla/5.0                 2594
# Mozilla/4.0                  601
# GoogleMaps/RochesterNY       121
# Opera/9.80                    34
# TEST_INTERNET_AGENT           24
# GoogleProducer                21
# Mozilla/6.0                    5
# BlackBerry8520/5.0.0.681       4
```

- a 필드에는 URL 단축을 실행하는 브라우저, 단말기, 어플리케이션에 대한 정보가 들어있음
- 문자열에서 첫 번째 토큰을 잘라내서 사용자 행동에 대한 또 다른 정보를 생성

```python
cframe = frame[frame.a.notnull()]
cframe["os"] = np.where(cframe["a"].str.contains("Windows"), "Windows", "Not Windows")
cframe["os"][:5]
# 0        Windows
# 1    Not Windows
# 2        Windows
# 3    Not Windows
# 4        Windows
```

- a 필드가 결측값이 아닌 데이터를 추출
- agent 문자열에 "Windows"를 포함하는지 여부에 따라 윈도우 사용자와 그렇지 않은 사용자를 구분

```python
by_tz_os = cframe.groupby(["tz", "os"])
# size함수로 그룹별 횟수 count를 진행
agg_counts = by_tz_os.size().unstack().fillna(0)
agg_counts[:10]
```

<img src="https://user-images.githubusercontent.com/58063806/123639090-d2c95c00-d85a-11eb-9f75-37b17438876e.png" width=40% />

```python
indexer = agg_counts.sum(1).argsort()
count_subset = agg_counts.take(indexer[-10:])
count_subset

# agg_counts.sum(1).nlargest(10)
# tz
# America/New_York       1251.0
#                         521.0
# America/Chicago         400.0
# America/Los_Angeles     382.0
# America/Denver          191.0
# Europe/London            74.0
# Asia/Tokyo               37.0
# Pacific/Honolulu         36.0
# Europe/Madrid            35.0
# America/Sao_Paulo        33.0
```

- 사용자의 합계가 많은 순서대로 10개 로우만 추출
  - nlargest 메서드로 동일한 작업이 가능

<img src="https://user-images.githubusercontent.com/58063806/123639381-1328da00-d85b-11eb-9c16-848fb598bf2b.png" width=35% />

```python
count_subset = count_subset.stack()
count_subset.name = "total"
count_subset = count_subset.reset_index()
count_subset[:10]
```

<img src="https://user-images.githubusercontent.com/58063806/123640996-b4fcf680-d85c-11eb-9f7a-41b9db6f6026.png" width=35%/>

```python
sns.barplot(x="total", y="tz", hue="os", data=count_subset)
```

- 각 시간대(tz)의 os별 분포

<img src="https://user-images.githubusercontent.com/58063806/123641288-102ee900-d85d-11eb-8a1d-b1c5a0275e25.png" width=60% />

```python
def norm_total(group):
    group["normed_total"] = group.total / group.total.sum()
    return group

results = count_subset.groupby("tz").apply(norm_total)

# 위의 과정과 동일한 작업
# g = count_subset.groupby("tz")
# results = count_subset.total / g.total.transform("sum")

sns.barplot(x="normed_total", y="tz", hue="os", data=results)
```

- 각 시간대(tz)에서 비율 확인을 위해 0 ~ 1로 정규화

<img src="https://user-images.githubusercontent.com/58063806/123641639-6d2a9f00-d85d-11eb-8389-6cc8ccd3c85f.png" width=60% />



### MovieLens의 영화 평점 데이터

- 영화 평점, 영화에 대한 정보, 사용자에 대한 정보가 포함되어 있는 데이터

```python
import pandas as pd

pd.options.display.max_rows = 10

unames = ["user_id", "gender", "age", "occupation", "zip"]
users = pd.read_table("datasets/movielens/users.dat", sep="::", header=None, names=unames)
```

<img src="https://user-images.githubusercontent.com/58063806/123782342-32ce0a00-d910-11eb-99dc-3d970936491a.png" width=30% />

```python
rnames = ["user_id", "movie_id", "rating", "timestamp"]
ratings = pd.read_table("datasets/movielens/ratings.dat", sep="::", header=None, names=rnames)
```

<img src="https://user-images.githubusercontent.com/58063806/123782439-4f6a4200-d910-11eb-8e38-48a1cac3483c.png" width=30% />

```python
mnames = ["movie_id", "title", "genres"]
movies = pd.read_table("datasets/movielens/movies.dat", sep="::", header=None, names=mnames)
```

<img src="https://user-images.githubusercontent.com/58063806/123782505-60b34e80-d910-11eb-915a-065a8bf3ef8d.png" width=50% />

```python
data = pd.merge(pd.merge(ratings, users), movies)
# join의 대상이되는 column을 지정
# pd.merge(pd.merge(ratings, users, on="user_id"), movies, on="movie_id")
```

- 모든 데이터를 하나의 테이블로 조인

<img src="https://user-images.githubusercontent.com/58063806/123782978-d61f1f00-d910-11eb-82cb-16b7758ee4a6.png" width=80% />

```python
mean_ratings = data.pivot_table("rating", index="title", columns="gender", aggfunc="mean")
mean_ratings
```

- 성별에 따른 각 영화별 평점의 평균

<img src="https://user-images.githubusercontent.com/58063806/123783180-0c5c9e80-d911-11eb-997c-df8043c36ec4.png" width=30% />

```python
ratings_by_title = data.groupby("title").size()
ratings_by_title.head(10)
# title
# $1,000,000 Duck (1971)                37
# 'Night Mother (1986)                  70
# 'Til There Was You (1997)             52
# 'burbs, The (1989)                   303
# ...And Justice for All (1979)        199
# 1-900 (1994)                           2
# 10 Things I Hate About You (1999)    700
# 101 Dalmatians (1961)                565
# 101 Dalmatians (1996)                364
# 12 Angry Men (1957)                  616

active_titles = ratings_by_title.index[ratings_by_title >= 250]
active_titles
# Index([''burbs, The (1989)', '10 Things I Hate About You (1999)',
#       '101 Dalmatians (1961)', '101 Dalmatians (1996)', '12 Angry Men (1957)',
#       '13th Warrior, The (1999)', '2 Days in the Valley (1996)',
#       '20,000 Leagues Under the Sea (1954)', '2001: A Space Odyssey (1968)',
#       '2010 (1984)',
#       ...
```

- 평점 정보가 250개 이상 존재하는 영화만 추출

```python
mean_ratings["diff"] = mean_ratings["M"] - mean_ratings["F"]
sorted_by_diff = mean_ratings.sort_values(by="diff")
sorted_by_diff.head(10)
```

- 남자와 여자의 평점차이를 기준으로 정렬

<img src="https://user-images.githubusercontent.com/58063806/123783692-9e64a700-d911-11eb-8a46-77f46fa78ac7.png" width=50%/>

```python
rating_std_by_title = data.groupby("title")["rating"].std()
rating_std_by_title = rating_std_by_title.loc[active_titles]

rating_std_by_title.sort_values(ascending=False)[:10]
# title
# Dumb & Dumber (1994)                     1.321333
# Blair Witch Project, The (1999)          1.316368
# Natural Born Killers (1994)              1.307198
# Tank Girl (1995)                         1.277695
# Rocky Horror Picture Show, The (1975)    1.260177
# Eyes Wide Shut (1999)                    1.259624
# Evita (1996)                             1.253631
# Billy Madison (1995)                     1.249970
# Fear and Loathing in Las Vegas (1998)    1.246408
# Bicentennial Man (1999)                  1.245533
```

- 표준편차를 이용해서 성별에 관계없이 호불호가 많이 갈리는 (사람들이 매긴 각 평점의 차이가 큰) 영화 추출 

