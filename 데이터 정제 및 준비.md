## 데이터 정제 및 준비

#### 개별화와 양자화

- 연속성 데이터는 종종 개별로 분할하거나 분석을 위해 그룹별로 나눔

```python
import pandas as pd

ages = [20, 22, 25, 27, 21, 23, 37, 31, 61, 45, 41, 32]
bins = [18, 25, 35, 60, 100]

cats = pd.cut(ages, bins)

# [(18, 25], (18, 25], (18, 25], (25, 35], (18, 25], ..., (25, 35], (60, 100], (35, 60], (35, 60], (25, 35]]
# Length: 12
# Categories (4, interval[int64]): [(18, 25] < (25, 35] < (35, 60] < (60, 100]]
```

- Categorical이라는 특수한 객체를 반환

```python
cats.codes
# array([0, 0, 0, 1, 0, 0, 2, 1, 3, 2, 2, 1], dtype=int8)
```

- ages 데이터에 대한 카테고리 인덱스

```python
cats.categories
# IntervalIndex([(18, 25], (25, 35], (35, 60], (60, 100]], closed='right', dtype='interval[int64]')
```

- codes 속성에 있는 ages 데이터에 대한 카테고리 이름
- 간격을 나타내는 표기는 중괄호로 시작해서 대괄호로 끝나는데 **중괄호 쪽의 값은 포함하지 않고 대괄호 쪽의 값은 포함**

```python
pd.cut(ages, [18, 26, 36, 61, 100], right=False)

# [[18, 26), [18, 26), [18, 26), [26, 36), [18, 26), ..., [26, 36), [61, 100), [36, 61), [36, 61), [26, 36)]
# Length: 12
# Categories (4, interval[int64]): [[18, 26) < [26, 36) < [36, 61) < [61, 100)]

group_names = ["Youth", "YoungAdult", "MiddleAged", "Senior"]

pd.cut(ages, bins, labels=group_names)

# [Youth, Youth, Youth, YoungAdult, Youth, ..., YoungAdult, Senior, MiddleAged, MiddleAged, YoungAdult]
# Length: 12
# Categories (4, object): [Youth < YoungAdult < MiddleAged < Senior]
```

- right=False 옵션으로 중괄호와 대괄호의 위치를 바꿀 수 있음

- labels 옵션으로 그룹의 이름을 직접지정 가능

```python
import numpy as np

data = np.random.rand(20)
# precision - 소수점 자리수
pd.cut(data, 4, precision=2)

# [(0.75, 0.99], (0.52, 0.75], (0.29, 0.52], (0.52, 0.75], (0.75, 0.99], ..., (0.06, 0.29], (0.06, 0.29], (0.75, 0.99], (0.29, 0.52], (0.29, 0.52]]
# Length: 20
# Categories (4, interval[float64]): [(0.06, 0.29] < (0.29, 0.52] < (0.52, 0.75] < (0.75, 0.99]]
```

- 그룹의 경계값을 넘기지 않고 그룹의 개수를 넘겨주면 데이터에서 최솟값과 최댓값을 기준으로 균등한 길이의 그룹을 자동으로 계산 

```python
data = np.random.randn(1000)

# 4분위로 분할
cats = pd.qcut(data, 4)

# [(-3.455, -0.704], (0.707, 3.526], (-0.704, 0.0376], (-3.455, -0.704], (0.0376, 0.707], ..., (0.707, 3.526], (0.707, 3.526], (-0.704, 0.0376], (0.0376, 0.707], (0.0376, 0.707]]
# Length: 1000
# Categories (4, interval[float64]): [(-3.455, -0.704] < (-0.704, 0.0376] < (0.0376, 0.707] < (0.707, 3.526]]

cats.value_counts()

# (-3.455, -0.704]    250
# (-0.704, 0.0376]    250
# (0.0376, 0.707]     250
# (0.707, 3.526]      250
```

- cut 함수를 사용하면 **데이터의 분산에 따라 각각의 그룹마다 데이터의 수가 다르게 나뉘는 경우가 많음**
- qcut은 **표준 변위치를 기반으로 데이터를 분할하기 때문에 적당히 같은 크기의 그룹으로 나눌 수 있음**

```python
pd.qcut(data, [0, 0.1, 0.5, 0.9, 1.])

# [(-3.455, -1.288], (1.276, 3.526], (-1.288, 0.0376], (-1.288, 0.0376], (0.0376, 1.276], ..., (1.276, 3.526], (0.0376, 1.276], (-1.288, 0.0376], (0.0376, 1.276], (0.0376, 1.276]]
# Length: 1000
# Categories (4, interval[float64]): [(-3.455, -1.288] < (-1.288, 0.0376] < (0.0376, 1.276] < (1.276, 3.526]]
```

- cut 함수와 유사하게 변위치(0 ~ 1)를 직접 지정 가능

#### 더미 변수

- 한 로우가 여러 카테고리에 속해서 get_dummies를 사용하지 못하는 경우

<img src="https://user-images.githubusercontent.com/58063806/118136770-6d9ee000-b43f-11eb-8d7a-03ea120f631b.png" width=50% />

```python
all_genres = []

for g in movies.genres:
    all_genres.extend(g.split("|"))
    
genres = pd.unique(all_genres)
genres

# array(['Animation', "Children's", 'Comedy', 'Adventure', 'Fantasy',        'Romance', 'Drama', 'Action', 'Crime', 'Thriller', 'Horror', 'Sci-Fi', 'Documentary', 'War', 'Musical', 'Mystery', 'Film-Noir', 'Western'], dtype=object)

zero_matrix = np.zeros((len(movies), len(genres)))
dummies = pd.DataFrame(zero_matrix, columns=genres)

dummies.head()
```

0으로 초기화된 Dataframe

<img src="https://user-images.githubusercontent.com/58063806/118137021-afc82180-b43f-11eb-991d-970eb1ec9761.png" width=90% />

```python
for i, gen in enumerate(movies.genres):
    # 데이터의 각 행을 돌면서 해당되는 장르(column)들에 1값 넣어줌 
    indices = dummies.columns.get_indexer(gen.split("|"))
    dummies.iloc[i, indices] = 1
dummies.head()
```

<img src="https://user-images.githubusercontent.com/58063806/118137360-16e5d600-b440-11eb-855d-ebb93215b4b2.png" width=90% />

```python
movies_windic = movies.join(dummies.add_prefix("Genre_"))
movies_windic.iloc[0]
```

기존의 movies Dataframe과 위에서 생성한 dummies의 column에 Genre_ 접두어를 붙인 Dataframe과 결합 

<img src="https://user-images.githubusercontent.com/58063806/118137676-717f3200-b440-11eb-8e45-b98699a6da66.png" width=30% />

get_dummies와 cut을 잘 조합하면 유용하게 사용 가능

```python
import numpy as np

np.random.seed(12345)
values = np.random.rand(10)
print(values)

# [0.92961609 0.31637555 0.18391881 0.20456028 0.56772503 0.5955447  0.96451452 0.6531771  0.74890664 0.65356987]

bins = [0, 0.2, 0.4, 0.6, 0.8, 1]

pd.get_dummies(pd.cut(values, bins))
```

<img src="https://user-images.githubusercontent.com/58063806/118137823-996e9580-b440-11eb-8cf7-c9e1e3a23ff0.png" width=40% />

#### 정규 표현식

- 텍스트에서 문자열 패턴을 찾는 유연한 방법을 제공
- 파이썬 내장 re 모듈 함수는 패턴 매칭, 치환, 분리 세 가지

```python
import re

text = "foo bar\t baz \tquz"
re.split("\s+", text)
# ['foo', 'bar', 'baz', 'quz']

regex = re.compile("\s+")
regex.split(text)
# ['foo', 'bar', 'baz', 'quz']

# 정규 표현식에 매칭되는 모든 패턴의 목록
regex.findall(text)
# [' ', '\t ', ' \t']
```

- **여러 가지 공백 문자(탭, 스페이스 , 개행 문자)가 포함된 문자열**을 나눌때는 **하나 이상의 공백 문자를 의미하는 \s+**를 사용
- 정규 표현식을 컴파일한 뒤 얻은 **정규 표현식 객체를 재사용하는 것도 가능**

```python
text = """Dave dave@google.com
Steve steve@gmail.com
Rob rob@gmail.com
Ryan ryan@yahoo.com
"""
pattern = r'[A-Z0-9._%+-]+@[A-Z0-9.-]+\.[A-Z]{2,4}'

# re.IGNORECASE는 정규 표현식이 대소문자를 가리지 않도록 함
regex = re.compile(pattern, flags=re.IGNORECASE)

regex.findall(text)
# ['dave@google.com', 'steve@gmail.com', 'rob@gmail.com', 'ryan@yahoo.com']

m = regex.search(text)
m
# <re.Match object; span=(5, 20), match='dave@google.com'>

text[m.start():m.end()]
# 'dave@google.com'
```

- search는 텍스트에서 첫 번째 이메일 주소만 찾아줌
- 정규 표현식에 대한 match 객체는 그 **정규 표현 패턴이 문자열 내에서 위치하는 시작점과 끝점**만을 알려줌

```python
print(regex.match(text))
# None

print(regex.sub("REDACTED", text))
# Dave REDACTED
# Steve REDACTED
# Rob REDACTED
# Ryan REDACTED
```

- regex.match는 정규 표현 패턴이 문자열의 시작점에서부터 일치하는지 검사하기 때문에 None 반환 (text는 Dave로 시작)
- sub 메서드는 찾은 패턴을 주어진 문자로 치환하여 새로운 문자열을 반환

```python
pattern = r'([A-Z0-9._%+-]+)@([A-Z0-9.-]+)\.([A-Z]{2,4})'

regex = re.compile(pattern, flags=re.IGNORECASE)

m = regex.match("wesm@bright.net")
# 각 패턴 컴포넌트의 튜플 반환
m.groups()
# ('wesm', 'bright', 'net')

regex.findall(text)
# [('dave', 'google', 'com'),
# ('steve', 'gmail', 'com'),
# ('rob', 'gmail', 'com'),
# ('ryan', 'yahoo', 'com')]
```

- 각 패턴을 괄호로 묶어줌으로써 이메일 주소를 찾아서 동시에 각 이메일 주소를 사용자 이름, 도메인 이름, 도메인 접미사 세가지 컴포넌트로 나눔

```python
print(regex.sub(r"Username: \1, Domain: \2, Suffix: \3", text))
# Dave Username: dave, Domain: google, Suffix: com
# Steve Username: steve, Domain: gmail, Suffix: com
# Rob Username: rob, Domain: gmail, Suffix: com
# Ryan Username: ryan, Domain: yahoo, Suffix: com
```

- \1은 첫 번째로 찾은 그룹, \2는 두 번째로 찾은 그룹, \3는 세 번째로 찾은 그룹을 의미 

```python
data = {"Dave": "dave@google.com", "Steve": "steve@gmail.com", "Rob": "rob@gmail.com", "Wes": np.nan}
data = pd.Series(data)
data
# Dave     dave@google.com
# Steve    steve@gmail.com
# Rob        rob@gmail.com
# Wes                  NaN
# dtype: object

data.str.contains("gmail")
# Dave     False
# Steve     True
# Rob       True
# Wes        NaN

pattern = r'([A-Z0-9._%+-]+)@([A-Z0-9.-]+)\.([A-Z]{2,4})'
data.str.findall(pattern, flags=re.IGNORECASE)
# Dave     [(dave, google, com)]
# Steve    [(steve, gmail, com)]
# Rob        [(rob, gmail, com)]
# Wes                        NaN

matches = data.str.match(pattern, flags=re.IGNORECASE)
matches
# Dave     True
# Steve    True
# Rob      True
# Wes       NaN

# 문자열 잘라냄
data.str[:5]
# Dave     dave@
# Steve    steve
# Rob      rob@g
# Wes        NaN
```

str.contains - 데이터들에 해당 문자열이 포함되어 있는지 검사(True, False 반환)



